# üìä Projeto de Dados - Recarga Pay

Este reposit√≥rio cont√©m a implementa√ß√£o de um **produto de dados** desenvolvido como parte de um teste t√©cnico. O objetivo principal √© **ingerir, processar e visualizar dados** utilizando PySpark, DuckDB e Streamlit.

---

## üì¶ Estrutura do Projeto

```
recarga-pay-matheus-cordeiro/
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ app.py                      # Aplica√ß√£o principal em Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ nb_libs.py                  # Instala e garante depend√™ncias
‚îÇ   ‚îú‚îÄ‚îÄ nb_duck_db.py               # Classe de integra√ß√£o com DuckDB
‚îÇ   ‚îú‚îÄ‚îÄ nb_dados_brutos.py          # Ingest√£o de dados brutos (camada bronze)
‚îÇ   ‚îú‚îÄ‚îÄ nb_hist_saldo_silver.py     # Processamento hist√≥rico de saldo (camada silver)
‚îÇ   ‚îú‚îÄ‚îÄ nb_saldo_juros_silver.py    # C√°lculo de juros sobre saldo (camada silver)
‚îÇ   ‚îú‚îÄ‚îÄ nb_valor_taxa.py            # Classe para atualizar taxa de juros dinamicamente
‚îÇ   ‚îî‚îÄ‚îÄ nb_tests_notebook.py        # Notebook/script de testes automatizados simples
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ artifacts/                      # Pasta destinada a armazenar arquivos temporarios do Spark (n√£o vercionada)
‚îú‚îÄ‚îÄ data/                           # Pasta destinada a o banco de dados DuckDB (n√£o versionada)
‚îú‚îÄ‚îÄ datalake/                       # Pasta destinada a arquivos de dados do Spark (n√£o versionada)
‚îú‚îÄ‚îÄ hadoop/                         # Pasta destinada aos arquivos de configuracao do Hadoop
‚îú‚îÄ‚îÄ interviews_fake_transactions/   # Pasta destinada aos arquivos de base disponibilizados para RecargaPay
‚îî‚îÄ‚îÄ README.md                       # Documenta√ß√£o principal
```

### üìñ Descri√ß√£o dos Notebooks / M√≥dulos

#### `app.py`

* Arquivo principal da aplica√ß√£o.
* Implementado em **Streamlit**, respons√°vel por criar a interface web.
* Permite visualizar dados processados a partir do DuckDB.
* Possui bot√µes para executar os scripts de ingest√£o e transforma√ß√£o (`nb_dados_brutos.py`, `nb_hist_saldo_silver.py`, `nb_saldo_juros_silver.py`).
* Inclui um **slider** para ajustar dinamicamente a taxa de juros via `nb_valor_taxa.py`.
* Exibe o **schema do banco** e permite executar queries SQL customizadas.

#### `nb_libs.py`

* Script utilit√°rio para garantir que todas as depend√™ncias est√£o instaladas.
* Instala/atualiza bibliotecas essenciais como `pandas`, `numpy`, `pyspark`, `duckdb`.
* Executado automaticamente pelo `app.py` no in√≠cio da aplica√ß√£o.

#### `nb_duck_db.py`

* Cont√©m a classe `DuckDB` que centraliza a integra√ß√£o com o banco **DuckDB**.
* Fun√ß√µes principais:

  * Criar banco e conectar.
  * Executar queries (`select_from_duckdb`).
  * Criar/atualizar tabelas com DataFrames do Spark.
  * Gravar dados via Parquet tempor√°rio.
  * Dropar tabelas e fechar conex√µes.
* Atua como camada de persist√™ncia de dados de consumo.

#### `nb_dados_brutos.py`

* Respons√°vel pela ingest√£o da camada **bronze**.
* L√™ arquivos **Parquet** de transa√ß√µes brutas (`interviews_fake_transactions`).
* Grava os dados na estrutura de **datalake/bronze/transacoes**.
* Persiste os dados no DuckDB na tabela `tb_transacoes`.
* Garante a cria√ß√£o da pasta e substitui√ß√£o dos dados em execu√ß√µes subsequentes.

#### `nb_hist_saldo_silver.py`

* Respons√°vel por criar a camada **silver** de **hist√≥rico de saldo**.
* L√™ a camada bronze de transa√ß√µes.
* Converte colunas para tipos corretos (ex: `amount`, `event_time`, `cdc_sequence_num`).
* Cria janela particionada por `account_id` para calcular **ordena√ß√£o temporal**.
* Gera hist√≥rico de movimenta√ß√µes (`df_hist`).
* Calcula **saldo acumulado** (`df_saldo`).
* Grava sa√≠da no **datalake/silver/historico\_saldo** e no DuckDB (`tb_saldo_historico`).

#### `nb_saldo_juros_silver.py`

* Respons√°vel por calcular **juros** sobre saldos (camada **silver**).
* L√™ dados do hist√≥rico de saldo (`silver/historico_saldo`).
* Calcula tempo desde a √∫ltima movimenta√ß√£o em horas (`hours_since_mov`).
* Aplica taxa de juros fixa (0.01) para saldos acima de 100 e com mais de 24h sem movimenta√ß√£o.
* Cria novo saldo atualizado (`updated_balance`).
* Grava sa√≠da em **datalake/silver/saldo\_juros** e no DuckDB (`tb_saldo_juros`).

#### `nb_valor_taxa.py`

* Define a classe `ValorTaxa` que permite **ajustar dinamicamente** a taxa de juros.
* Instanciada com a taxa definida pelo usu√°rio (via Streamlit).
* Executa o mesmo fluxo de c√°lculo de juros do `nb_saldo_juros_silver.py`, mas aplicando a taxa escolhida.
* Atualiza os dados no **datalake/silver/saldo\_juros** e na tabela DuckDB `tb_saldo_juros`.

#### `nb_tests_notebook.py`

* Cont√©m **testes automatizados simples** para validar o pipeline.
* Principais testes inclu√≠dos:

  * Criar e consultar tabela no DuckDB.
  * Calcular saldo acumulado com janela de parti√ß√£o.
  * Executar `ValorTaxa` com taxa de 10% e validar a cria√ß√£o da coluna `fees`.
* Exibe mensagens de sucesso para cada teste.

---

## ‚öôÔ∏è Instala√ß√£o

### Pr√©-requisitos

* Python 3.10+
* [Poetry](https://python-poetry.org/) ou `pip`
* DuckDB >= 0.9
* PySpark >= 3.5
* Streamlit >= 1.35

### Spark

* **Java**: Instalar Java 8 ou Java 11: https://www.azul.com/downloads/?package=jdk#download-openjdk.
* **Vari√°vel de ambiente do Java:** 
```bash
setx JAVA_HOME "C:\Program Files\Zulu\zulu-11"
```

### Hadoop
* **Baixar o Hadoop 3.3.1**: https://github.com/cdarlint/winutils.
* **Pasta Hadoop**: Deixei a pasta dentro do projeto para facilitar na configuracao mais nao e o idea! O mais correto era criar uma pasta em: `C:\hadoop` 


### Instala√ß√£o com `pip`

```bash
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
.venv\Scripts\activate      # Windows

pip install -r requirements.txt
```

### Instala√ß√£o com `poetry`

```bash
poetry install
```

---

## ‚ñ∂Ô∏è Execu√ß√£o

1. **Configurar os dados**
   Coloque seus arquivos de dados grandes (CSV/Parquet) na pasta `data/`.

2. **Executar o Streamlit**

   ```bash
   streamlit run notebooks/app.py
   ```

3. **Interface**
   A aplica√ß√£o abrir√° no navegador em `http://localhost:8501`.

---

## üß™ Testes

Este projeto inclui um script/notebook de testes: `notebooks/nb_tests_notebook.py`.

### Como executar os testes

```bash
python notebooks/nb_tests_notebook.py
```

### Testes inclu√≠dos

* **Teste DuckDB**: cria e consulta tabela `tb_teste`.
* **Teste de saldo acumulado**: valida c√°lculo de saldo progressivo.
* **Teste de juros com ValorTaxa**: aplica taxa de 10% e verifica coluna `fees`.

Esses testes s√£o b√°sicos, mas garantem o funcionamento essencial do pipeline. Para produ√ß√£o, recomenda-se evoluir com `pytest` e frameworks de qualidade de dados como **Great Expectations**.

---

## üóÑÔ∏è Recomenda√ß√µes para Ingest√£o em Banco Transacional

* **Produ√ß√£o**: evitar DuckDB como banco final, usar PostgreSQL ou SQL Server.
* **Ingest√£o em lote**:

  * Exportar dados processados para arquivos Parquet particionados.
  * Utilizar ferramenta de orquestra√ß√£o (Airflow/Databricks Jobs) para carga incremental.
* **Ingest√£o em streaming**:

  * Usar Spark Structured Streaming conectado a Kafka ou EventHub.
  * Persistir em tabelas de staging antes da normaliza√ß√£o.
* **Boas pr√°ticas**:

  * Definir chaves prim√°rias e √≠ndices adequados.
  * Controlar versionamento de dados (SCD tipo 2).
  * Monitorar qualidade dos dados antes da carga.

---

## üèóÔ∏è Escolhas de Design

### Funcionais

* **PySpark**: utilizado para manipula√ß√£o de grandes volumes de dados com efici√™ncia.
* **DuckDB**: escolhido como banco anal√≠tico local para persist√™ncia leve e consultas r√°pidas.
* **Streamlit**: fornece visualiza√ß√£o simples e interativa para explorar os resultados.

### N√£o Funcionais

* **Escalabilidade**: arquitetura modular que permite troca de banco (DuckDB ‚Üí PostgreSQL) facilmente.
* **Performance**: leitura em Parquet e uso de Spark para paralelismo.
* **Manutenibilidade**: c√≥digo separado em m√≥dulos para ingest√£o, transforma√ß√£o e consumo.

---

## ‚öñÔ∏è Trade-offs e Compromissos

* **DuckDB em vez de PostgreSQL**: escolhido pela simplicidade no teste, mas n√£o ideal para ambiente produtivo.
* **Dados n√£o versionados no reposit√≥rio**: arquivos grandes foram omitidos para n√£o inflar o GitHub.
* **Interface simples em Streamlit**: priorizada a entrega funcional em vez de design sofisticado.
* **Aus√™ncia de testes complexos**: devido a restri√ß√£o de tempo, apenas testes b√°sicos foram contemplados.

---

‚úçÔ∏è **Autor:** Matheus Rodrigues
- [LinkedIn](https://www.linkedin.com/in/matheus-rodrigues-106319b7/)  
- [E-mail](mailto:matheus.rodrigues_santos@hotmail.com)
