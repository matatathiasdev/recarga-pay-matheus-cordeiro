# üìä Projeto de Dados - Recarga Pay

Este reposit√≥rio cont√©m a implementa√ß√£o de um **produto de dados** desenvolvido como parte de um teste t√©cnico. O objetivo principal √© **ingerir, processar e visualizar dados** utilizando PySpark, DuckDB e Streamlit.

---

## üì¶ Estrutura do Projeto

```
recarga-pay-matheus-cordeiro/
‚îÇ
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Aplica√ß√£o principal Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ nb_duck_db.py       # Classe de integra√ß√£o com DuckDB
‚îÇ   ‚îú‚îÄ‚îÄ nb_valor_taxa.py    # M√≥dulo para grava√ß√£o e leitura de taxas
‚îÇ   ‚îî‚îÄ‚îÄ ...                 # Outros notebooks de apoio
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias do projeto
‚îú‚îÄ‚îÄ README.md               # Documenta√ß√£o principal
‚îî‚îÄ‚îÄ data/                   # Pasta destinada a arquivos de dados (n√£o versionada)
```

---

## ‚öôÔ∏è Instala√ß√£o

### Pr√©-requisitos

* Python 3.10+
* [Poetry](https://python-poetry.org/) ou `pip`
* DuckDB >= 0.9
* PySpark >= 3.5
* Streamlit >= 1.35

### Instala√ß√£o com `pip`

```bash
python -m venv .venv
source .venv/bin/activate   # Linux/Mac
.venv\Scripts\activate      # Windows

pip install -r requirements.txt
```

### Instala√ß√£o com `poetry`

```bash
poetry install
```

---

## ‚ñ∂Ô∏è Execu√ß√£o

1. **Configurar os dados**
   Coloque seus arquivos de dados grandes (CSV/Parquet) na pasta `data/`.

2. **Executar o Streamlit**

   ```bash
   streamlit run notebooks/app.py
   ```

3. **Interface**
   A aplica√ß√£o abrir√° no navegador em `http://localhost:8501`.

---

## üß™ Testes

Testes simples podem ser feitos executando:

```bash
pytest -v
```

*(se os testes unit√°rios estiverem configurados em `tests/`)*

---

## üóÑÔ∏è Recomenda√ß√µes para Ingest√£o em Banco Transacional

* **Produ√ß√£o**: evitar DuckDB como banco final, usar PostgreSQL ou SQL Server.
* **Ingest√£o em lote**:

  * Exportar dados processados para arquivos Parquet particionados.
  * Utilizar ferramenta de orquestra√ß√£o (Airflow/Databricks Jobs) para carga incremental.
* **Ingest√£o em streaming**:

  * Usar Spark Structured Streaming conectado a Kafka ou EventHub.
  * Persistir em tabelas de staging antes da normaliza√ß√£o.
* **Boas pr√°ticas**:

  * Definir chaves prim√°rias e √≠ndices adequados.
  * Controlar versionamento de dados (SCD tipo 2).
  * Monitorar qualidade dos dados antes da carga.

---

## üèóÔ∏è Escolhas de Design

### Funcionais

* **PySpark**: utilizado para manipula√ß√£o de grandes volumes de dados com efici√™ncia.
* **DuckDB**: escolhido como banco anal√≠tico local para persist√™ncia leve e consultas r√°pidas.
* **Streamlit**: fornece visualiza√ß√£o simples e interativa para explorar os resultados.

### N√£o Funcionais

* **Escalabilidade**: arquitetura modular que permite troca de banco (DuckDB ‚Üí PostgreSQL) facilmente.
* **Performance**: leitura em Parquet e uso de Spark para paralelismo.
* **Manutenibilidade**: c√≥digo separado em m√≥dulos (`nb_duck_db`, `nb_valor_taxa`).

---

## ‚öñÔ∏è Trade-offs e Compromissos

* **DuckDB em vez de PostgreSQL**: escolhido pela simplicidade no teste, mas n√£o ideal para ambiente produtivo.
* **Dados n√£o versionados no reposit√≥rio**: arquivos grandes foram omitidos para n√£o inflar o GitHub.
* **Interface simples em Streamlit**: priorizada a entrega funcional em vez de design sofisticado.
* **Aus√™ncia de testes complexos**: devido a restri√ß√£o de tempo, apenas testes b√°sicos foram contemplados.

---

‚úçÔ∏è **Autor:** Matheus Rodrigues
